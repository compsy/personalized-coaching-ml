{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script builts 5 different predictive models for an individual for the whole period of available data. The algorithms are based on the results of notebook 2. LinearSVC,AdaBoost Classifier,Logistic Regression, MLPClassifier, and SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect oracle using windows client..\n",
    "#comment:download correct version:3.5 python+ Oracle 12c  windows 64\n",
    "#download and pip install cx_Oracle-5.2+oci12c-cp35-none-win_amd64.whl from lfd.uci.edu/~gohlke/pythonlibs/\n",
    "#cx_oracle \n",
    "#commect:Download the right oracle client 12c+ windows 64 and install in C:\\instantclient_12_1\n",
    "#http://www.oracle.com/technetwork/topics/winx64soft-089540.html\n",
    "#Set path in windows add C:\\instantclient_12_1 (use path manager.exe) restart\n",
    "\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# classifiers & testing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import itertools\n",
    "import re  \n",
    "import os \n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import json\n",
    "import collections\n",
    "from math import sqrt\n",
    "from random import randrange\n",
    "from random import seed\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "import warnings\n",
    "from sklearn.preprocessing import Binarizer, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures, RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.feature_selection import SelectFwe, SelectKBest, SelectPercentile, VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import itertools\n",
    "\n",
    "con = cx_Oracle.connect('hanzefitbit/hanzefitbit@127.0.0.1/xe')\n",
    "\n",
    "\n",
    "#con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to determine if people have reached their goal of the day\n",
    "\n",
    "def daily_steps_cat_f (steps_value,threshold):\n",
    "    if (steps_value<threshold):\n",
    "        #print('smaller then threshold')\n",
    "        return 0\n",
    "    if (steps_value>=threshold):\n",
    "        #print('more then threshold')\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model_f (dest,treatment_id,algorithm_name):\n",
    "    pickle_model=str(treatment_id)+'_'+algorithm_name+'_'+'model.pkl' \n",
    "    #if algorithm_name=='SVC':\n",
    "    #    pickle.dump(globals()['clf%s' % algorithm_name+treatment_id],open(os.path.join(dest,pickle_model),'wb'),protocol=4)    \n",
    "    #else:    \n",
    "    pickle.dump(globals()['clf%s' % algorithm_name+treatment_id].best_estimator_,open(os.path.join(dest,pickle_model),'wb'),protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickle_destination_f (root,leaf):\n",
    "    dest = os.path.join(root,leaf) \n",
    "    if not os.path.exists(dest): \n",
    "        os.makedirs(dest) \n",
    "    return dest    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_same(items):\n",
    "    return all(x == items[0] for x in items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,parameter_name):\n",
    "    parameter_id=find_parameter_id_f(algorithm_name,parameter_name)\n",
    "    if parameter_name=='C':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.C\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='max_features':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.max_features\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)    \n",
    "    if parameter_name=='criterion':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.criterion\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='penalty':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.penalty\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    #if parameter_name=='min_impurity_decrease':\n",
    "        #value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.min_impurity_decrease\n",
    "        #insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)    \n",
    "    if parameter_name=='n_estimators':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.n_estimators\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)     \n",
    "    if parameter_name=='fit_intercept':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.fit_intercept\n",
    "        value=str(value)\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)  \n",
    "    if parameter_name=='learning_rate':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.learning_rate\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='alpha':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.alpha\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='binarize':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.binarize\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)   \n",
    "    if parameter_name=='fit_prior':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.fit_prior\n",
    "        value=str(value)\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='class_prior':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.class_prior\n",
    "        value=str(value)\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='loss':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.loss\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='dual':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.dual\n",
    "        value=str(value)\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='activation':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.activation\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='learning_rate_init':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.learning_rate_init\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name=='kernel':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.kernel\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)    \n",
    "    if parameter_name=='l1_ratio':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.l1_ratio\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)    \n",
    "    if parameter_name =='metric':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.metric\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name =='weights':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.weights\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)\n",
    "    if parameter_name =='n_neighbors':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.n_neighbors\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value) \n",
    "    if parameter_name =='loss':\n",
    "        value=globals()['clf%s' % algorithm_name+treatment_id].best_estimator_.loss\n",
    "        insert_into_model_parameters_f(model_id,treatment_id,parameter_id,value)   \n",
    "        \n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model_database_f(treatment_id,pickle_model,algorithm_name,dest):\n",
    "    #Save description of models into hft_models_t\n",
    "        sql = \"SELECT HFT_MODEL_T_ID_SEQ.NEXTVAL FROM DUAL\"\n",
    "        cur.execute(sql)\n",
    "        #cur.fetchone [0] returns value,cur.fetchone returns tuple\n",
    "        model_id=cur.fetchone()[0]\n",
    "        sql = \"INSERT INTO HFT_MODEL_T(ID,NAME,ALGORITHM,DESTINATION) VALUES (:1,:2,:3,:4)\"\n",
    "        cur.execute(sql,(model_id,pickle_model,algorithm_name,dest))\n",
    "        con.commit()\n",
    "        \n",
    "         #insert the tuned parameter values into the database    \n",
    "        if algorithm_name=='LR':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'C')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'penalty')\n",
    "            insert_hyperparameter_into_database_f(model_id,treatment_id,algorithm_name,'fit_intercept')\n",
    "        elif algorithm_name=='ADA':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'learning_rate')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'n_estimators')  \n",
    "        elif algorithm_name =='RF':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'criterion')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'max_features') \n",
    "            #insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'min_impurity_decrease')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'n_estimators') \n",
    "        elif algorithm_name=='NN':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'learning_rate')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'activation')\n",
    "            insert_hyperparameter_into_database_f(model_id,treatment_id,algorithm_name,'learning_rate_init')\n",
    "        elif algorithm_name=='SGD':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'fit_intercept')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'l1_ratio')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'loss')\n",
    "        elif algorithm_name=='SVC':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'kernel')\n",
    "        elif algorithm_name =='DT':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'criterion')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'max_features') \n",
    "        elif algorithm_name =='KNN':\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'metric')\n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'weights') \n",
    "            insert_hyperparameter_into_database_f (model_id,treatment_id,algorithm_name,'n_neighbors') \n",
    "        \n",
    "        #save the perfomance of the models into the hft_metrics_t \n",
    "        #get values of confusion matrix\n",
    "        globals()['df_confusion_matrix%s' % treatment_id]=pd.DataFrame(metrics.confusion_matrix(globals()['y%s' % treatment_id], globals()['y_pred%s' % algorithm_name+treatment_id]))\n",
    "          \n",
    "        #threshold\n",
    "        THRESHOLD=int(globals()['df_threshold%s' % treatment_id]['avg_daily_steps'])\n",
    "       \n",
    "       \n",
    "        #true negative\n",
    "        TN = int(globals()['df_confusion_matrix%s' % treatment_id].iloc[0,0])\n",
    "\n",
    "               \n",
    "        #true positive\n",
    "        FP = int(globals()['df_confusion_matrix%s' % treatment_id].iloc[0,1])\n",
    "\n",
    "        #false negative\n",
    "        FN = int(globals()['df_confusion_matrix%s' % treatment_id].iloc[1,0])\n",
    "\n",
    "        #true positive\n",
    "        TP = int(globals()['df_confusion_matrix%s' % treatment_id].iloc[1,1])\n",
    "\n",
    "        #f1 score\n",
    "        F1 = float(f1_score(globals()['y%s' % treatment_id], globals()['y_pred%s' % algorithm_name+treatment_id], average='macro'))\n",
    "         \n",
    "        #Accuracy\n",
    "        ACC = float(metrics.accuracy_score(globals()['y%s' % treatment_id], globals()['y_pred%s' % algorithm_name+treatment_id]))\n",
    "        \n",
    "        #number of observations\n",
    "              \n",
    "        OBS=int(len(globals()['df%s' % treatment_id].index))\n",
    "        \n",
    "        # -1 is for all weekdays\n",
    "        WEEKDAY=-1\n",
    "        #insert results into database\n",
    "        sql=\"INSERT INTO HFT_METRICS_T(F1_SCORE,TRUE_NEGATIVE,TRUE_POSITIVE,FALSE_NEGATIVE,FALSE_POSITIVE,HFT_TREATMENT_ID,HFT_MODEL_ID,THRESHOLD,ACCURACY,NUMBER_OF_OBSERVATIONS,WEEKDAY)\\\n",
    "        VALUES (:1,:2,:3,:4,:5,:6,:7,:8,:9,:10,:11)\"\n",
    "        cur.execute(sql,(F1,TN,TP,FN,FP,treatment_id,model_id,THRESHOLD,ACC,OBS,WEEKDAY))\n",
    "        con.commit()\n",
    "         \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_model (treatment_id,use_algorithm_name,use_algorithm,X_train,y_train):\n",
    "    treatment_id_name=str(treatment_id)\n",
    "    \n",
    "    \n",
    "    if use_algorithm_name =='LR':\n",
    "        #use_algorithm = LogisticRegression()\n",
    "        #parameters GridSearch Logistic Regression\n",
    "        clf__penalty_param= ['l1', 'l2']\n",
    "        clf__C_param= [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "        clf_fit_intercept=[True, False]\n",
    "        parameters = {'C':clf__C_param, 'penalty':clf__penalty_param,'fit_intercept':clf_fit_intercept} \n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name == 'ADA':\n",
    "        clf_learning_rate=[0.1, 0.5, 1.0, 10.0]\n",
    "        clf_n_estimators=[10, 50]\n",
    "        parameters = {'learning_rate':clf_learning_rate, 'n_estimators':clf_n_estimators}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name =='RF':\n",
    "        n_estimators_values = [10, 50, 100, 500]\n",
    "        #min_impurity_decrease_values = [0., 0.005, 0.00025]\n",
    "        max_features_params = [0.1, 0.25, 0.5, 0.75, 'sqrt', 'log2', None]\n",
    "        criterion_params = ['gini', 'entropy']\n",
    "        parameters = {'criterion':criterion_params, 'max_features':max_features_params,'n_estimators':n_estimators_values}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters,error_score=0.0, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name == 'DT':\n",
    "        criterion_params = ['gini','entropy']\n",
    "        max_features_params = ['auto','sqrt','log2'] \n",
    "        parameters = {'criterion':criterion_params, 'max_features':max_features_params}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters,error_score=0.0, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name=='NN':\n",
    "        learning_rate=['constant', 'invscaling', 'adaptive']\n",
    "        activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "        learningrate_params = [0.01, 0.05, 0.1, 0.5, 1.0] \n",
    "        parameters = {'learning_rate':learning_rate,'activation':activation,'learning_rate_init':learningrate_params}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name=='SGD':\n",
    "        fit_intercept=[True, False]\n",
    "        l1_ratio=[0, 0.15, 1.0]\n",
    "        loss_params=['log','modified_huber']\n",
    "        parameters = {'fit_intercept':fit_intercept,'l1_ratio':l1_ratio,'loss':loss_params}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name=='SVC':\n",
    "        print(treatment_id_name)\n",
    "        kernel=['sigmoid','rbf']\n",
    "        parameters = {'kernel':kernel}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    elif use_algorithm_name=='KNN':\n",
    "        metric_params = ['minkowski','euclidean','manhattan'] \n",
    "        weights_params = ['uniform','distance'] \n",
    "        numNeighbors_params  = [5, 6, 7, 8, 9]\n",
    "        parameters = {'metric':metric_params,'weights':weights_params,'n_neighbors':numNeighbors_params}\n",
    "        globals()['clf%s' % use_algorithm_name + treatment_id_name] = GridSearchCV(use_algorithm, parameters, cv=5).fit(X_train, y_train)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find parameter id for saving the results of gridsearch\n",
    "def find_parameter_id_f (algorithm_name,parameter_name):\n",
    "    cur=con.cursor()\n",
    "    cur.execute('SELECT ID FROM HFT_PARAMETERS_T WHERE HFT_ALGORITHM_T_NAME like:algorithm_name AND NAME like:parameter_name',algorithm_name=algorithm_name,parameter_name=parameter_name)\n",
    "    parameter_id=cur.fetchone()[0]\n",
    "    ##print (cur.fetchone())\n",
    "    return parameter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert tuned parameters per model into database\n",
    "def insert_into_model_parameters_f (model_id,treatment_id,parameter_id,value):\n",
    "    cur=con.cursor()\n",
    "    sql='INSERT INTO HFT_MODEL_PARAMETERS_T(HFT_MODEL_T_ID,HFT_PARAMETERS_T_ID,TREATMENT_ID,VALUE) VALUES(:1,:2,:3,:4)'\n",
    "    cur.execute(sql,(model_id,parameter_id,treatment_id,value))\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select treatment id's for looping through all models etc...\n",
    "cur = con.cursor()\n",
    "cur.execute('select distinct treatment_id from hft_sum_steps_v where research_group in (1,2) and weekday not in (5,6)')\n",
    "#dataframe\n",
    "df_treatment_id=pd.DataFrame(cur.fetchall(),columns=['treatment_id'])\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select data from measurements\n",
    "#added case to select only the weeks where coaching took place research group 1 started in week 5 with coaching, research group 2 started in week 16 with coaching\n",
    "cur = con.cursor()\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    cur.execute('select id,treatment_id,year,week,weekday,hour,sum_steps, sum_steps_hour,daily_steps from hft_sum_steps_v where \\\n",
    "             treatment_id=:treatment_id and hour in (7,8,9,10,11,12,13,14,15,16,17,18) \\\n",
    "             and weekday not in (5,6) and year=2015 and (case when research_group=2 and week>15 then 1 when research_group=1 and week>4 then 1 else 0 end = 1)\\\n",
    "             order by year,week,weekday,hour',treatment_id=treatment_id)     \n",
    "    globals()['df%s' % treatment_id]= pd.DataFrame(cur.fetchall(),columns=['id','treatment_id','year','week','weekday','hour','sum_steps','sum_steps_hour','daily_steps'])\n",
    "#cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select avg steps per day \n",
    "#cur = con.cursor()\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    cur.execute( 'select treatment_id,avg(sum_steps_hour) avg_daily_steps from hft_sum_steps_v where treatment_id=:treatment_id\\\n",
    "                  and year=2015 and hour=18 and weekday not in (5,6) and (case when research_group=2 and week>15 then 1 when research_group=1 and week>4 then 1 else 0 end = 1)\\\n",
    "                  group by treatment_id',treatment_id=treatment_id)\n",
    "    globals()['df_threshold%s' % treatment_id]= pd.DataFrame(cur.fetchall(),columns=['treatment_id','avg_daily_steps'])\n",
    "#cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set threshold\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    globals()['threshold%s' % treatment_id] = globals()['df_threshold%s' % treatment_id]['avg_daily_steps']\n",
    "    #print(globals()['df_threshold%s' % treatment_id].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use numpy vectorize to use function with more arguments and a dataframe determine if the threshold is met\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    x=globals()['df%s' % treatment_id]['daily_steps']\n",
    "    y=globals()['df_threshold%s' % treatment_id]['avg_daily_steps']\n",
    "    globals()['df%s' % treatment_id]['dailysteps_cat']=np.vectorize(daily_steps_cat_f)(x, y)\n",
    "    #print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split df \n",
    "from sklearn.model_selection import train_test_split\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    X= globals()['df%s' % treatment_id].iloc[:, 5:8].values\n",
    "    y= globals()['df%s' % treatment_id].iloc[:, 9].values\n",
    "    #normalize for NN,SVC,SGD and KNN\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(X)\n",
    "    X_s = pd.DataFrame(np_scaled)\n",
    "    \n",
    "    globals()['X_train%s' % treatment_id],globals()['X_test%s' % treatment_id],globals()['y_train%s' % treatment_id],globals()['y_test%s' % treatment_id]\\\n",
    "    =train_test_split(X,y, test_size=0.3, random_state=10)    \n",
    "    \n",
    "    globals()['X_train_s%s' % treatment_id],globals()['X_test_s%s' % treatment_id],globals()['y_train%s' % treatment_id],globals()['y_test%s' % treatment_id]\\\n",
    "    =train_test_split(X,y, test_size=0.3, random_state=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#built models for different algorithms\n",
    "#import warnings\n",
    "#with warnings.catch_warnings():\n",
    "    #warnings.simplefilter('ignore')\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    if all_same(globals()['y_train%s' % treatment_id]):\n",
    "        pass\n",
    "    else:\n",
    "        fit_model(treatment_id,'ADA',AdaBoostClassifier(),globals()['X_train%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'DT',DecisionTreeClassifier(),globals()['X_train%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'KNN',KNeighborsClassifier(),globals()['X_train_s%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'LR',LogisticRegression(),globals()['X_train%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'NN',MLPClassifier(),globals()['X_train_s%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'RF',RandomForestClassifier(),globals()['X_train%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'SGD', SGDClassifier(),globals()['X_train_s%s' % treatment_id],globals()['y_train%s' % treatment_id])\n",
    "        fit_model(treatment_id,'SVC',SVC(probability=True),globals()['X_train_s%s' % treatment_id],globals()['y_train%s' % treatment_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prediction for different models\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=int(i)\n",
    "    if all_same(globals()['y_train%s' % treatment_id]):\n",
    "        pass\n",
    "    else:\n",
    "        \n",
    "        globals()['X%s' % treatment_id]= globals()['df%s' % treatment_id].iloc[:, 5:8].values\n",
    "        globals()['y%s' % treatment_id]= globals()['df%s' % treatment_id].iloc[:,9].values\n",
    "        \n",
    "        #scale for NN,SVC,SGD and KNN\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        np_scaled = min_max_scaler.fit_transform(globals()['X%s' % treatment_id])\n",
    "        globals()['X_s%s' % treatment_id] = pd.DataFrame(np_scaled)\n",
    "        \n",
    "        \n",
    "        globals()['y_predADA%s' % treatment_id] = globals()['clfADA%s' % treatment_id].predict(globals()['X%s' % treatment_id])\n",
    "        globals()['y_predDT%s' % treatment_id] = globals()['clfDT%s' % treatment_id].predict(globals()['X%s' % treatment_id])\n",
    "        globals()['y_predKNN%s' % treatment_id] = globals()['clfKNN%s' % treatment_id].predict(globals()['X_s%s' % treatment_id])\n",
    "        globals()['y_predLR%s' % treatment_id] = globals()['clfLR%s' % treatment_id].predict(globals()['X%s' % treatment_id])\n",
    "        globals()['y_predNN%s' % treatment_id] = globals()['clfNN%s' % treatment_id].predict(globals()['X_s%s' % treatment_id])\n",
    "        globals()['y_predRF%s' % treatment_id] = globals()['clfRF%s' % treatment_id].predict(globals()['X%s' % treatment_id])\n",
    "        globals()['y_predSGD%s' % treatment_id] = globals()['clfSGD%s' % treatment_id].predict(globals()['X_s%s' % treatment_id])\n",
    "        globals()['y_predSVC%s' % treatment_id] = globals()['clfSVC%s' % treatment_id].predict(globals()['X_s%s' % treatment_id])     \n",
    "          \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save all models on file system and the desciption and the metrics in the database\n",
    "dest = pickle_destination_f ('app','pkl_objects')\n",
    "for i in df_treatment_id['treatment_id']:\n",
    "    treatment_id=str(i)\n",
    "    if all_same(globals()['y_train%s' % treatment_id]):\n",
    "        pass\n",
    "    else:\n",
    "        algorithm_name='LR'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "\n",
    "        algorithm_name='ADA'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "              \n",
    "        algorithm_name='RF'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "        \n",
    "              \n",
    "        algorithm_name='DT'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "        \n",
    "        algorithm_name='KNN'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "        \n",
    "        algorithm_name='NN'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "                \n",
    "        algorithm_name='SGD'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "        \n",
    "        algorithm_name='SVC'\n",
    "        pickle_model=str((treatment_id)+'_'+algorithm_name+'_'+'model.pkl')\n",
    "        save_model_f(dest,treatment_id,algorithm_name)\n",
    "        save_model_database_f(treatment_id,pickle_model,algorithm_name,dest)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
